# -*- coding: utf-8 -*-
"""Sentiment-Android.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gVoDz4ebbdvn94ybSNRh_GKEmmKD7jqG
"""

#Rumus Slovin untuk Play Store
# Diketahui
N = 117000  # Jumlah populasi (total review)
e = 0.05  # Margin of error (5%)

# Menghitung ukuran sampel dengan rumus Slovin
n = N / (1 + N * e**2)
n

!pip install google-play-scraper pandas

from google_play_scraper import reviews, Sort
import pandas as pd

app_package = "in.startv.hotstar.dplus"

#Scraping review
reviews_data, _ = reviews(
    app_package,
    count=399,
    lang="id",
    country="id",
    sort=Sort.NEWEST
)

df_playstore = pd.DataFrame(reviews_data)

df_playstore.to_csv("android-reviews.csv", index=False)
print("Scraping selesai! Hasil telah disimpan di android-reviews.csv")

df_playstore.head()

!pip install googletrans==4.0.0-rc1

from googletrans import Translator

translator = Translator()

input_file = 'android-reviews.csv'
data = pd.read_csv(input_file)

df = pd.DataFrame(data)
if 'content' not in df.columns:
  raise KeyError("Kolom 'content' tidak ditemukan.")

#Translate kolom content
def translate_text(text):
  try:
    translated = translator.translate(text, src='id', dest='en')
    return translated.text
  except Exception as e:
    print(f"Terjadi kesalahan saat menerjemahkan teks: {e}")

df['content_translated'] = df['content'].apply(translate_text)

df.to_csv('android-reviews-translated.csv', index=False)
print("Terjemahan selesai! Hasil telah disimpan di android-reviews-translated.csv")

input_file = 'android-reviews-translated.csv'
data = pd.read_csv(input_file)

def case_folding(text):
  try:
    return text.lower()
  except AttributeError:
    return text

df['content_case_folding'] = df['content_translated'].apply(case_folding)

df.to_csv('android-reviews-case-folding.csv', index=False)
print("Case folding selesai! Hasil telah disimpan di android-reviews-case-folding.csv")

input_file = 'android-reviews-case-folding.csv'
data = pd.read_csv(input_file)

def tokenize_text(text):
  try:
    return text.split()
  except AttributeError:
    return[]

df['content_tokenized'] = df['content_case_folding'].apply(tokenize_text)

df.to_csv('android-reviews-tokenized.csv', index=False)
print("Tokenisasi selesai! Hasil telah disimpan di android-reviews-tokenized.csv")

import nltk
from nltk.corpus import stopwords
import re

nltk.download('stopwords')

def remove_stopwords(text):
  stop_words = set(stopwords.words('english'))

  if isinstance(text, str):
    text = text.strip('[]')
    words = re.findall(r'\'([^\']+)\'|"([^"]+)"', text)
    words = [w[0] or w[1] for w in words if w[0] or w[1]]
  else:
    words = text

  cleaned_text = []
  for word in words:
    word = word.strip().strip("'").strip('"').strip()
    if word and word.lower() not in stop_words:
      cleaned_text.append(word)
  return cleaned_text

df = pd.read_csv('android-reviews-tokenized.csv')

df['stopwords_removed'] = df['content_tokenized'].apply(remove_stopwords)

df.to_csv('android-reviews-stopword.csv', index=False)
print("Stopword removal selesai! Hasil telah disimpan di android-reviews-stopword.csv")

import ast

input_file = 'android-reviews-stopword.csv'
data = pd.read_csv(input_file)

def to_lowercase(content):
  try:
    # Use ast.literal_eval to safely parse the string representation of a list
    words = ast.literal_eval(content)
    return str([word.lower() for word in words])
  except (AttributeError, SyntaxError, ValueError): # Handling potential errors during parsing
    return content

df['final'] = df['stopwords_removed'].apply(to_lowercase)

df.to_csv('android-reviews-final.csv', index=False)
print("Lowercasing selesai! Hasil telah disimpan di android-reviews-final.csv")

from transformers import pipeline

data_file = 'android-reviews-stopword.csv'
df = pd.read_csv(data_file)

sentiment_analyzer = pipeline('sentiment-analysis')

def predict_sentiment(text):
  result = sentiment_analyzer(text)[0]
  return 1 if result['label'] == 'POSITIVE' else 0

df['sentiment'] = df['stopwords_removed'].apply(predict_sentiment)

df.to_csv('android-reviews-sentiment.csv', index=False)
print("Sentiment analysis selesai! Hasil telah disimpan di android-reviews-sentiment.csv")

data_file = 'android-reviews-sentiment.csv'
df = pd.read_csv(data_file)

def labeling(rating):
  if rating in [1,2]:
    return 0
  if rating in [4,5]:
    return 1
  else:
    return None #Handle Neutral scores like 3

df['score_label'] = df['score'].apply(labeling)

df.to_csv('android-reviews-labeled.csv', index=False)
print("Labeling selesai! Hasil telah disimpan di android-reviews-labeled.csv")

import seaborn as sns
import matplotlib.pyplot as plt

data_file = 'android-reviews-labeled.csv'
df = pd.read_csv(data_file)

# Count the occurrences of each label
label_counts = df['score_label'].value_counts()

correlation_matrix = df[['score_label', 'sentiment']].corr()

# Create a heatmap
plt.figure(figsize=(8, 6))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

output_file = 'android-reviews-sentiment.csv'
df = pd.read_csv(output_file)

sentiment_counts = df['sentiment'].value_counts().sort_index()

sentiment_labels = ['Bad(0)', 'Good(1)']

plt.bar(sentiment_labels, sentiment_counts, color=['red', 'green'])
plt.xticks(rotation=45)
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.title('Sentiment Distribution')

for i, count in enumerate(sentiment_counts):
  plt.text(i, count, str(count), ha='center', va='bottom')

plt.tight_layout()
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

output_file = 'android-reviews-sentiment.csv'
df = pd.read_csv(output_file)

good_sentiments = df[df['sentiment'] == 1]['stopwords_removed']

text = ' '.join(good_sentiments.dropna())

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Good Sentiments WordCloud')
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

output_file = 'android-reviews-sentiment.csv'
df = pd.read_csv(output_file)

bad_sentiments = df[df['sentiment'] == 0]['stopwords_removed']

text = ' '.join(bad_sentiments.dropna())

wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='coolwarm').generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Bad Sentiments WordCloud')
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Combine all rows of 'final_content' into a single string
# Convert lists back to strings and concatenate them
all_text = ' '.join([' '.join(eval(row)) for row in df['stopwords_removed'] if isinstance(row, str)])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Plot the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud from 'final' Column", fontsize=16)
plt.show()

from collections import Counter
import numpy as np

all_text = ' '.join([' '.join(eval(row)) for row in df['stopwords_removed'] if isinstance(row,str)])

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

freq_word = wordcloud.process_text(all_text)

word_counts = Counter(freq_word)

top_words = word_counts.most_common(10)

words, counts = zip(*top_words)

plt.figure(figsize=(10, 6))
plt.bar(words, counts, color='skyblue')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Top 10 Most Frequent Words')
plt.xticks(rotation=0)

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re

# Load the CSV file (replace with your actual file path)
data_file = 'android-reviews-labeled.csv'
df = pd.read_csv(data_file)

# List of words to exclude (case insensitive)
custom_stopwords = [
    "devices", "connection", "use", "downloaded", "installed", "verification",
    "entered", "register", "delete", "signal", "proxy", "restart", "subscription",
    "update", "said", "help", "look", "tried", "times", "reason", "turns", "many",
    "always", "continue", "stop", "right", "instead", "next", "several", "beginning",
    "lot", "around", "feature", "basic", "must", "payment", "balance", "auto",
    "period", "refund", "premium", "price", "thousand", "paying", "return", "money",
    "cost", "buy", "fund", "film", "watched", "play", "movies", "subtitles", "soundtrack",
    "smooth", "visual", "subscribing", "fund", "paid", "bug", "quota", "cellphone",
    "cut", "star", "week", "long", "hot", "improve", "service", "automatically",
    "immediately", "cancel", "internet", "list", "fix", "send", "added", "go",
    "okay", "try", "told", "black", "offline", "watch", "expensive", "cheap",
    "free", "monthly", "days", "error", "voice", "often", "really", "even",
    "though", "still", "already"
]
# Combine all the text from the 'text_without_stopwords' column
text = ' '.join(df['stopwords_removed'].dropna())

# Clean the text by removing unwanted characters like punctuation
text = re.sub(r"[^\w\s]", "", text)  # Remove punctuation using regex

# Split the text into words and filter out the excluded words (case insensitive)
words = [word.lower() for word in text.split() if word.lower() not in custom_stopwords]

# Generate Word Cloud from filtered words
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))

# Plot the Word Cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Remove axis
plt.title("Word Cloud of Text Without Specific Words")
plt.show()

import pandas as pd
import ast
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Load data
df = pd.read_csv('android-reviews-final.csv')

# Convert text list safely into an actual list
def safe_eval(row):
    try:
        return ast.literal_eval(row)
    except (ValueError, SyntaxError):
        return []

df['final'] = df['final'].apply(safe_eval)

# Join words into full sentences
documents = [' '.join(words) for words in df['final'] if isinstance(words, list)]

# Custom stop words (tambahkan kata umum yang tidak perlu)
custom_stopwords = ['watch', 'disney', 'application', 'apk', 'use', 'really',
                    'want', 'download', 'like', 'can', 'still', 'even', 'already', 'please', 'though', 'also', 'first', 'one', 'go', 'thank', 'make', 'it']

# Vectorizer with LDA-friendly settings
vectorizer = CountVectorizer(max_df=0.85, min_df=3, stop_words=custom_stopwords)
X = vectorizer.fit_transform(documents)

# LDA Model (ubah jumlah topik jika diperlukan)
lda_model = LatentDirichletAllocation(n_components=6, random_state=42)
lda_model.fit(X)

# Print top words for each topic
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(lda_model.components_):
    top_words_idx = topic.argsort()[:-10 - 1:-1]  # Get top 10 words
    print(f"Topic {topic_idx + 1}: {' '.join(feature_names[top_words_idx])}")

# Lihat probabilitas dokumen untuk tiap topik
doc_topic_dist = lda_model.transform(X)

# Cek topik dominan untuk tiap dokumen
dominant_topics = doc_topic_dist.argmax(axis=1)

# Hitung distribusi topik
import numpy as np
topic_counts = np.bincount(dominant_topics)

# Print jumlah dokumen dalam tiap topik
for i, count in enumerate(topic_counts):
    print(f"Topic {i+1}: {count} documents")

"""📌 Analisis Akhir

1. Topic 1: Masalah login & error aplikasi
→ (log in, login, difficult, error, hard, quality, correct). ✅


2. Topic 2: Langganan & pemakaian data
→ (pay, subscribe, data, watching, error). ✅


3. Topic 3: OTP & harga paket langganan
→ (code, otp, enter, netflix, expensive, price). ✅


4. Topic 4: Tampilan layar & kualitas suara/video
→ (black screen, sound, video, picture, film, money). ✅


5. Topic 5: Masalah loading & Hotstar
→ (watching, movie, loading, garbage, hotstar). ✅


6. Topic 6: VPN, koneksi internet & pembayaran
→ (subscription, vpn, wifi, paid, watching). ✅
"""